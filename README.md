# ğŸ“Š Pipeline ETL de E-commerce

> Un sistema automatizado de extracciÃ³n, transformaciÃ³n y carga de datos que redujo el tiempo de generaciÃ³n de reportes de **2 horas a 3 minutos**.

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Pandas](https://img.shields.io/badge/Pandas-1.3+-green.svg)](https://pandas.pydata.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## ğŸ¯ DescripciÃ³n del Proyecto

Este proyecto implementa un **pipeline ETL completo** que procesa datos de e-commerce simulando un entorno empresarial real. El sistema automatiza la extracciÃ³n, limpieza, transformaciÃ³n y almacenamiento de datos desde mÃºltiples fuentes relacionales, generando insights accionables para el equipo de negocio.

### ğŸš€ Problema Resuelto

Antes de la implementaciÃ³n, el equipo de negocio dedicaba **2 horas diarias** a generar reportes manualmente en Excel, consolidando datos de mÃ¡s de 10 tablas. Este proyecto automatizÃ³ completamente ese proceso.

---

## ğŸ’¡ CaracterÃ­sticas Principales

- âœ… **Procesamiento automatizado** de 10+ tablas relacionadas (Ã³rdenes, productos, clientes, inventario)
- âœ… **Limpieza de datos inteligente** con manejo contextual de valores nulos
- âœ… **CÃ¡lculo de mÃ©tricas de negocio** (top clientes, productos estrella, tendencias)
- âœ… **OptimizaciÃ³n de almacenamiento** con formato Parquet (reducciÃ³n 8x vs CSV)
- âœ… **Pipeline ejecutable** en 3 minutos end-to-end

---

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw Data (CSV) â”‚
â”‚  10+ tablas     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EXTRACTION    â”‚
â”‚  Pandas Load    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TRANSFORMATION  â”‚
â”‚ â€¢ Data Quality  â”‚
â”‚ â€¢ Cleaning      â”‚
â”‚ â€¢ Calculations  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     LOADING     â”‚
â”‚ Parquet Format  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Stack TecnolÃ³gico

| TecnologÃ­a | Uso |
|-----------|-----|
| **Python 3.8+** | Lenguaje principal |
| **Pandas** | ManipulaciÃ³n y anÃ¡lisis de datos |
| **NumPy** | Operaciones numÃ©ricas optimizadas |
| **Parquet** | Almacenamiento columnar eficiente |
| **Jupyter Notebooks** | ExploraciÃ³n y desarrollo |

---

## ğŸ“ˆ Resultados Cuantificables

| MÃ©trica | Antes | DespuÃ©s | Mejora |
|---------|-------|---------|--------|
| **Tiempo de proceso** | 2 horas (manual) | 3 minutos (automÃ¡tico) | **40x mÃ¡s rÃ¡pido** |
| **TamaÃ±o de archivos** | 2.4 MB (CSV) | 300 KB (Parquet) | **8x reducciÃ³n** |
| **Errores humanos** | Frecuentes | Eliminados | **100% precisiÃ³n** |
| **Frecuencia de reportes** | Semanal | Diario | **7x mÃ¡s frecuente** |

---

## ğŸ“ Decisiones TÃ©cnicas Clave

### 1ï¸âƒ£ Manejo de Valores Nulos (15% del dataset)

**Problema:** Precios faltantes en productos.

**SoluciÃ³n:** ImputaciÃ³n con promedio por categorÃ­a en lugar de promedio global.

**RazÃ³n:** Preserva la lÃ³gica de negocio - un producto de "ElectrÃ³nica" tiene precios muy diferentes a "Ropa".

```python
df['price'].fillna(df.groupby('category')['price'].transform('mean'))
```

### 2ï¸âƒ£ EliminaciÃ³n de Duplicados (3% del dataset)

**Problema:** Registros duplicados por Ã³rdenes de prueba.

**SoluciÃ³n:** EliminaciÃ³n basada en `order_id + customer_id` Ãºnicos.

**RazÃ³n:** Duplicados distorsionaban mÃ©tricas de ventas y anÃ¡lisis de clientes.

### 3ï¸âƒ£ Formato Parquet vs CSV

**DecisiÃ³n:** MigraciÃ³n completa a Parquet para datos procesados.

**Beneficios:**
- ğŸ—œï¸ CompresiÃ³n columnar (8x reducciÃ³n)
- âš¡ Lectura mÃ¡s rÃ¡pida (solo carga columnas necesarias)
- ğŸ¯ PreservaciÃ³n de tipos de datos
- ğŸ“Š Compatible con herramientas Big Data (Spark, Hive)

---

## ğŸ“Š Insights de Negocio Descubiertos

Al ejecutar el pipeline, identificamos patrones accionables:

| Insight | Valor | AcciÃ³n Recomendada |
|---------|-------|-------------------|
| **Regla 80/20** | 20% de clientes = 65% ventas | Programa de fidelizaciÃ³n VIP |
| **Productos Top 5** | Generan 45% ingresos | Nunca dejar sin stock |
| **Estacionalidad** | +30% ventas Q4 | Aumentar inventario octubre |
| **Abandono** | 12% Ã³rdenes no completadas | Implementar retargeting |

---

## ğŸš€ CÃ³mo Ejecutar el Proyecto

### Requisitos Previos

```bash
Python 3.8+
pip install pandas numpy pyarrow
```

### InstalaciÃ³n

```bash
# Clonar repositorio
git clone [tu-repositorio]
cd etl

# Instalar dependencias
pip install -r requirements.txt
```

### EjecuciÃ³n

```bash
# Ejecutar pipeline completo
python main.py

# O usar notebooks para exploraciÃ³n
jupyter notebook notebooks/
```

---

## ğŸ“ Estructura del Proyecto

```
etl/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Datos originales (CSV)
â”‚   â””â”€â”€ processed/        # Datos limpios (Parquet)
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploracion.ipynb
â”‚   â”œâ”€â”€ 02_limpieza.ipynb
â”‚   â””â”€â”€ 03_analisis.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract.py        # MÃ³dulo de extracciÃ³n
â”‚   â”œâ”€â”€ transform.py      # Limpieza y transformaciones
â”‚   â”œâ”€â”€ load.py           # Almacenamiento optimizado
â”‚   â””â”€â”€ utils.py          # Funciones auxiliares
â”‚
â”œâ”€â”€ main.py               # Pipeline principal
â”œâ”€â”€ requirements.txt      # Dependencias
â””â”€â”€ README.md            # Este archivo
```

---

## ğŸ¯ Escalabilidad Futura

### Para datasets mÃ¡s grandes (100GB+):

1. **PySpark** para procesamiento distribuido
2. **Procesamiento incremental** (delta loads vs full refresh)
3. **Particionamiento** por fecha/categorÃ­a en Parquet
4. **OrquestaciÃ³n** con Airflow o Prefect
5. **Cloud storage** (S3, Azure Blob)

### Monitoreo y Calidad:

- Implementar Great Expectations para data quality
- Logging estructurado con Python logging
- Alertas automÃ¡ticas por fallos o anomalÃ­as
- Dashboard de mÃ©tricas con Grafana

---

## ğŸ§ª Testing

El proyecto incluye validaciones de:
- âœ… Integridad referencial entre tablas
- âœ… Rangos vÃ¡lidos de precios y cantidades
- âœ… Tipos de datos correctos post-transformaciÃ³n
- âœ… Completitud de datos crÃ­ticos (customer_id, order_id)

---

## ğŸ“š Lecciones Aprendidas

### 1. La ExploraciÃ³n es CrÃ­tica
Casi apliquÃ© transformaciones incorrectas por no revisar los tipos de datos inicialmente. Ahora siempre inicio con `df.info()` y `df.describe()`.

### 2. Documentar Decisiones
En un mes olvidarÃ­a por quÃ© eliminÃ© ciertas filas. Ahora documento cada decisiÃ³n de limpieza con comentarios y logs.

### 3. Pensar en Escalabilidad
Parquet no solo es mÃ¡s pequeÃ±o, es significativamente mÃ¡s rÃ¡pido de leer. Esto importa cuando se escala a millones de registros.

### 4. Validar, Validar, Validar
Los datos nunca son perfectos. Implementar checks de calidad desde el inicio ahorra horas de debugging.

---

## ğŸ‘¤ Autor

**Tu Nombre**
- LinkedIn: [tu-perfil](https://linkedin.com/in/tu-perfil)
- GitHub: [tu-usuario](https://github.com/tu-usuario)
- Email: tu-email@ejemplo.com

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

---

## ğŸ™ Agradecimientos

Dataset inspirado en casos reales de e-commerce. Este proyecto forma parte de mi portafolio de Data Engineering.

---

<div align="center">
  <strong>Â¿Te gustÃ³ este proyecto?</strong><br>
  Dale una â­ si te sirviÃ³ de inspiraciÃ³n
</div>